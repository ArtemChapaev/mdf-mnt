{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "314d05e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "314d05e7",
        "outputId": "5d25c13a-86b4-436d-9229-cb7d52970d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting razdel\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install razdel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "r7IxI-ScLusj",
      "metadata": {
        "id": "r7IxI-ScLusj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForPreTraining, BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "59c49e30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59c49e30",
        "outputId": "f98e1ff6-6d9d-46f3-91ab-3662ae0ea132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "22790625",
      "metadata": {
        "id": "22790625"
      },
      "outputs": [],
      "source": [
        "DATA_PATH_PREFIX = 'drive/MyDrive/diploma/data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8288deef",
      "metadata": {
        "id": "8288deef"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = DATA_PATH_PREFIX + 'texts_for_align/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b5ac2ab",
      "metadata": {
        "id": "9b5ac2ab"
      },
      "source": [
        "# align and save"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6PE6xdDZLhVD"
      },
      "id": "6PE6xdDZLhVD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ZzMNPPypLusm",
      "metadata": {
        "id": "ZzMNPPypLusm"
      },
      "outputs": [],
      "source": [
        "model_path = 'drive/MyDrive/diploma/labse_moksha_40k+70k+2k_ce_0602_v2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "PmRxzirdE5Kb",
      "metadata": {
        "id": "PmRxzirdE5Kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a6a50b38-6178-49ed-8bd6-2de7c4bf9994"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForPreTraining(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(65143, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (cls): BertPreTrainingHeads(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=65143, bias=True)\n",
              "    )\n",
              "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model = AutoModelForPreTraining.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda();"
      ],
      "metadata": {
        "id": "iwsWXR2FbwOH"
      },
      "id": "iwsWXR2FbwOH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mdf_ru_texts(key: str) -> tuple[str, str]:\n",
        "    # проверяем, что все ключи релевантны\n",
        "    if not key.startswith('mdf') and not key.startswith('ru'):\n",
        "        raise RuntimeError(\"\")\n",
        "\n",
        "    # избежать повторную обработку, обрабатываем только если key начинается с mdf\n",
        "    if not key.startswith('mdf'):\n",
        "        return '', ''\n",
        "\n",
        "    # если ключ составной, то используем idx (key_parts[1]). иначе просто берем 'mdf', 'ru'\n",
        "    key_parts = key.split('_')\n",
        "    if len(key_parts) == 1:\n",
        "        return texts['mdf'], texts['ru']\n",
        "\n",
        "    if len(key_parts) == 2:\n",
        "        return texts[f'mdf_{key_parts[1]}'], texts[f'ru_{key_parts[1]}']\n",
        "\n",
        "    raise RuntimeError()\n",
        "\n",
        "\n",
        "def align_wikisource_doc(filename, print_non_parallel_texts:bool=False):\n",
        "    with open(filename, 'r') as f:\n",
        "        texts = json.load(f)\n",
        "\n",
        "    all_aligned_pairs = []\n",
        "\n",
        "    for key in texts.keys():\n",
        "        mdf_text, ru_text = get_mdf_ru_texts(key)\n",
        "\n",
        "        if not ru_text or not mdf_text:\n",
        "            print(f\"empty pair: ({key}), {mdf_text}, {ru_text}\")\n",
        "            continue\n",
        "\n",
        "        aligned_pairs = align_sentences(mdf_text, ru_text, model, tokenizer)\n",
        "        all_aligned_pairs += aligned_pairs\n",
        "\n",
        "        if print_non_parallel_texts and not aligned_pairs:\n",
        "            print(f\"0 aligned pairs: {key}, {mdf_text}, {ru_text}\")\n",
        "\n",
        "    return all_aligned_pairs"
      ],
      "metadata": {
        "id": "9M4oKU8sLneJ"
      },
      "id": "9M4oKU8sLneJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "578d037e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "578d037e",
        "outputId": "f08c2296-eae4-40a3-b162-f8d1b291ef0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "book='Mother_gorkiy'\n",
            "len(all_aligned_pairs)=1754\n",
            "\n",
            "book='gogol'\n",
            "len(all_aligned_pairs)=221\n",
            "\n",
            "book='vilage_fire_shchedrin'\n",
            "len(all_aligned_pairs)=144\n",
            "\n",
            "book='75.000_chekhov'\n",
            "len(all_aligned_pairs)=133\n",
            "\n",
            "book='chapaev'\n",
            "len(all_aligned_pairs)=813\n",
            "\n",
            "book='distant_countries_gaydar'\n",
            "len(all_aligned_pairs)=1135\n",
            "\n",
            "book='konyaga_shchedrin'\n",
            "len(all_aligned_pairs)=109\n",
            "\n",
            "book='Maria_Ivanovna_chekhov'\n",
            "len(all_aligned_pairs)=32\n",
            "\n",
            "book='self-delusion_chekhov'\n",
            "len(all_aligned_pairs)=36\n",
            "\n",
            "book='trifon_chekhov'\n",
            "len(all_aligned_pairs)=108\n",
            "\n",
            "book='italian_11_gorkiy'\n",
            "len(all_aligned_pairs)=89\n",
            "\n",
            "book='happiness_gorkiy'\n",
            "len(all_aligned_pairs)=54\n",
            "\n",
            "book='chameleon_chekhov'\n",
            "len(all_aligned_pairs)=92\n",
            "\n",
            "book='hero_gorkiy'\n",
            "len(all_aligned_pairs)=82\n",
            "\n",
            "book='land_dicret'\n",
            "len(all_aligned_pairs)=5\n",
            "\n",
            "book='Mother_gorkiy_part2'\n",
            "len(all_aligned_pairs)=263\n",
            "\n",
            "book='person_in_case_chekhov'\n",
            "len(all_aligned_pairs)=202\n",
            "\n",
            "book='prishibaev_chekhov'\n",
            "len(all_aligned_pairs)=88\n",
            "\n",
            "book='russian_gorkiy'\n",
            "len(all_aligned_pairs)=191\n",
            "\n",
            "book='konstitution'\n",
            "len(all_aligned_pairs)=373\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for filename in os.listdir(DATA_DIR):\n",
        "    if not filename.endswith('json'):\n",
        "        continue\n",
        "\n",
        "    book = '.'.join(filename.split('.')[:-1])\n",
        "    print(f\"{book=}\")\n",
        "\n",
        "    all_aligned_pairs = align_wikisource_doc(DATA_DIR + filename)\n",
        "    print(f\"{len(all_aligned_pairs)=}\")\n",
        "\n",
        "    data = []\n",
        "    for mdf, ru in all_aligned_pairs:\n",
        "        if not is_text_valid(mdf) or not is_text_valid(ru):\n",
        "            continue\n",
        "\n",
        "        data.append({'mdf': mdf, 'ru': ru})\n",
        "\n",
        "    with open(DATA_PATH_PREFIX + f'aligned_{book}_sents_06_02.json', \"w\") as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pYfPHlRNhUKO"
      },
      "id": "pYfPHlRNhUKO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}